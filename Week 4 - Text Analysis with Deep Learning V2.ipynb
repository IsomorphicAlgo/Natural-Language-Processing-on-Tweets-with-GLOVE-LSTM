{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micha\\AppData\\Local\\Temp\\ipykernel_27964\\636766028.py:26: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import RandomSearch\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "# Deep learning\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import keras_tuner\n",
    "from kerastuner import RandomSearch\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    f1_score, \n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    print(\"Downloading required NLTK data...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we want to crack into the data and see what we're working with\n",
    "train_data_frame = pd.read_csv('train.csv')\n",
    "\n",
    "train_data_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind lets check back in on the purpose of the Kaggle Competition.</br>\n",
    "</br>\n",
    "https://www.kaggle.com/c/nlp-getting-started/overview</br>\n",
    "</br>\n",
    "In this competition, you're challenged to build a machine learning model that predicts which Tweets are about real disasters and which one's aren't. You'll have access to a dataset of 10,000 tweets that were hand classified. If this is your first time working on an NLP problem, we've created a quick tutorial to get you up and running.</br>\n",
    "</br>\n",
    "Disclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.</br>\n",
    "</br>\n",
    "Now we want to state the class guidelines:</br>\n",
    "</br>\n",
    "Deliverable 1</br>\n",
    "</br>\n",
    "A Jupyter notebook with a description of the problem/data, exploratory data analysis (EDA) procedure, analysis (model building and training), result, and discussion/conclusion.</br>\n",
    "</br>\n",
    "Suppose your work becomes so large that it doesn't fit into one notebook (or you think it will be less readable by having one large notebook). In that case, you can make several notebooks or scripts in a GitHub repository (as deliverable 3) and submit a report-style notebook or pdf instead.</br>\n",
    "</br>\n",
    "If your project doesn't fit into Jupyter notebook format (E.g., you built an app that uses ML), write your approach as a report and submit it in a pdf form.</br>\n",
    "</br>\n",
    "Deliverable 2</br>\n",
    "</br>\n",
    "A public project GitHub repository with your work (please also include the GitHub repo URL in your notebook/report).</br>\n",
    "</br>\n",
    "Deliverable 3</br>\n",
    "</br>\n",
    "A screenshot of your position on the Kaggle competition leaderboard for your top-performing model.</br>\n",
    "</br>\n",
    "Step Breakdown:</br>\n",
    "</br>\n",
    "Step 1</br>\n",
    "Brief description of the problem and data (5 pts)</br>\n",
    "</br>\n",
    "Step 2</br>\n",
    "Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data (15 pts)</br>\n",
    "</br>\n",
    "Step 3</br>\n",
    "Model Architecture (25 pts)</br>\n",
    "</br>\n",
    "Step 4</br>\n",
    "Results and Analysis (35 pts)</br>\n",
    "</br>\n",
    "Step 5</br>\n",
    "Discussion and Conclusion (15 pts)</br>\n",
    "</br>\n",
    "Extra Rules described above in the deliverables section</br>\n",
    "Produce Deliverables: High-Quality, Organized Jupyter Notebook Report, GitHub Repository, and screenshot of Kaggle leaderboard (30 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For my approach I am going to do some simple Data exploration and make sure everythings all clean. Then we'll clean out the filler words and run the standard data statistics on our variables. Then we'll construct a word cloud for an extra layer of analysis.\n",
    "</br>\n",
    "\n",
    "Next we'll split up our data and then prepare for oru LSTM model. Once this is finished we will explore a TF-IDF model and see how it compares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n",
      "7613\n",
      "7613\n"
     ]
    }
   ],
   "source": [
    "# quick clean\n",
    "print(len(train_data_frame))\n",
    "# 1. remove duplicates\n",
    "train_data_frame = train_data_frame.drop_duplicates()\n",
    "print(len(train_data_frame))\n",
    "# 2. remove empty rows\n",
    "train_data_frame = train_data_frame[train_data_frame['text'].notna()]\n",
    "print(len(train_data_frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point it looks like the data matches the implied level of quality infered by the Kaggle Contest page and the data sets 'data card'.\n",
    "Next I want to address the two rows displayed above in the head(). The 'keyword' and 'location' columns display NaN, and this could complicate things. Lets chek and see if any data is present. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             count\n",
      "keyword           \n",
      "fatalities      45\n",
      "deluge          42\n",
      "armageddon      42\n",
      "damage          41\n",
      "body%20bags     41\n",
      "harm            41\n",
      "sinking         41\n",
      "evacuate        40\n",
      "outbreak        40\n",
      "fear            40\n",
      "                 count\n",
      "location              \n",
      "USA                104\n",
      "New York            71\n",
      "United States       50\n",
      "London              45\n",
      "Canada              29\n",
      "Nigeria             28\n",
      "UK                  27\n",
      "Los Angeles, CA     26\n",
      "India               24\n",
      "Mumbai              22\n",
      "2.902929200052542 % of rows contain values in the keyword column\n",
      "43.885459083147246 % of rows contain values in the location column\n"
     ]
    }
   ],
   "source": [
    "# check the distribution of values in the keyword column\n",
    "kayword_counts = pd.DataFrame(train_data_frame['keyword'].value_counts())\n",
    "kayword_counts = kayword_counts.sort_values(by='count', ascending=False)\n",
    "print(kayword_counts.head(10))\n",
    "\n",
    "# check the distribution of values in the location column\n",
    "location_counts = pd.DataFrame(train_data_frame['location'].value_counts())\n",
    "location_counts = location_counts.sort_values(by='count', ascending=False)\n",
    "print(location_counts.head(10))\n",
    "\n",
    "# percentage of missing values of rows in 'keyword' and 'location' columns \n",
    "print((221/len(train_data_frame))*100, '% of rows contain values in the keyword column')\n",
    "print((3341/len(train_data_frame))*100, '% of rows contain values in the location column')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is some issues with the fact that only 2% of the rows contain values in the keyword column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# next we remove the stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "more_stopwords = ['u', 'im', 'c']\n",
    "stop_words = stop_words + more_stopwords\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_frame = pd.read_csv('test.csv')\n",
    "test_data_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to process and clean tweets\n",
    "\n",
    "def preprocess_data(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords and stem\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    text = ' '.join(stemmer.stem(word) for word in tokens if word not in stop_words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to perform spell check\n",
    "def correct_spelling(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = []\n",
    "    for word in text.split():\n",
    "        corrected_word = spell.correction(word)\n",
    "        if corrected_word is not None:\n",
    "            corrected_words.append(corrected_word)\n",
    "        else:\n",
    "            corrected_words.append(word)  # Keep the original word if correction is None\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we handle emojis\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to display the mean number of words in a tweet in each row of the dataset\n",
    "def mean_words(text):\n",
    "    return len(text.split(' '))\n",
    "\n",
    "# calculate the mean number of words in the dataset\n",
    "train_data_frame['mean_words']=train_data_frame['text'].apply(lambda x: mean_words(x))\n",
    "test_data_frame['mean_words']=test_data_frame['text'].apply(lambda x: mean_words(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\micha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>mean_words</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>deed reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>forest fire near la long sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>reside ask shelter place notify office each sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>people receive wildfire each order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>got sent photo rub alaska smoke wildfire pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  mean_words                                         clean_text  \n",
       "0       1          13        deed reason earthquake may allah forgive us  \n",
       "1       1           7               forest fire near la long sask canada  \n",
       "2       1          22  reside ask shelter place notify office each sh...  \n",
       "3       1           9      people receive wildfire each order california  \n",
       "4       1          17  got sent photo rub alaska smoke wildfire pour ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt_tab')\n",
    "train_data_frame['clean_text']=train_data_frame['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# remove stopwords  \n",
    "train_data_frame['clean_text']=train_data_frame['clean_text'].apply(lambda x: preprocess_data(x))\n",
    "\n",
    "# remove extra whitespace\n",
    "train_data_frame['clean_text']=train_data_frame['clean_text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "# Spellcheck\n",
    "train_data_frame['clean_text'] = train_data_frame['clean_text'].apply(correct_spelling)\n",
    "train_data_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>mean_words</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>6</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>9</td>\n",
       "      <td>heard earthquake differ city stay safe everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>19</td>\n",
       "      <td>forest fire spot pond gees flee across street ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>4</td>\n",
       "      <td>apocalypse light spoken wildfire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>8</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash   \n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...   \n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...   \n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires   \n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan   \n",
       "\n",
       "   mean_words                                         clean_text  \n",
       "0           6                          happen terrible car crash  \n",
       "1           9    heard earthquake differ city stay safe everyone  \n",
       "2          19  forest fire spot pond gees flee across street ...  \n",
       "3           4                   apocalypse light spoken wildfire  \n",
       "4           8                 typhoon soudelor kill china taiwan  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_frame['clean_text']=test_data_frame['text'].apply(lambda x: remove_emoji(x))\n",
    "\n",
    "# remove stopwords  \n",
    "test_data_frame['clean_text']=test_data_frame['clean_text'].apply(lambda x: preprocess_data(x))\n",
    "\n",
    "# remove extra whitespace\n",
    "test_data_frame['clean_text']=test_data_frame['clean_text'].apply(lambda x: re.sub(r'\\s+', ' ', x))\n",
    "\n",
    "# Spellcheck\n",
    "test_data_frame['clean_text'] = test_data_frame['clean_text'].apply(correct_spelling)\n",
    "\n",
    "test_data_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>mean_words</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>deed reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>forest fire near la long sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>reside ask shelter place notify office shelter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>people receive wildfire order california</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>got sent photo rub alaska smoke wildfire pour ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  mean_words                                         clean_text  \n",
       "0       1          13        deed reason earthquake may allah forgive us  \n",
       "1       1           7               forest fire near la long sask canada  \n",
       "2       1          22  reside ask shelter place notify office shelter...  \n",
       "3       1           9           people receive wildfire order california  \n",
       "4       1          17  got sent photo rub alaska smoke wildfire pour ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# remove stopwords  \n",
    "train_data_frame['clean_text']=train_data_frame['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "test_data_frame['clean_text']=test_data_frame['clean_text'].apply(lambda x: remove_stopwords(x))\n",
    "train_data_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = train_data_frame['clean_text'].values\n",
    "test_tweets = test_data_frame['clean_text'].values\n",
    "train_target = train_data_frame['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f11_score_metric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    precision = tf.keras.metrics.Precision()(y_true, y_pred)\n",
    "    recall = tf.keras.metrics.Recall()(y_true, y_pred)\n",
    "    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_metric(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(tf.greater(y_pred, 0.5), tf.float32)\n",
    "    \n",
    "    true_pos = tf.reduce_sum(y_true * y_pred)\n",
    "    false_pos = tf.reduce_sum((1 - y_true) * y_pred)\n",
    "    false_neg = tf.reduce_sum(y_true * (1 - y_pred))\n",
    "    \n",
    "    precision = true_pos / (true_pos + false_pos + tf.keras.backend.epsilon())\n",
    "    recall = true_pos / (true_pos + false_neg + tf.keras.backend.epsilon())\n",
    "    \n",
    "    f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11464"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(train_tweets)\n",
    "\n",
    "vocab_length = len(word_tokenizer.word_index) + 1\n",
    "vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm.tqdm(df['clean_text']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop_words))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7613/7613 [00:00<00:00, 21274.63it/s]\n"
     ]
    }
   ],
   "source": [
    "train_corpus = create_corpus(pd.DataFrame({'clean_text': train_tweets}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(corpus): \n",
    "    return word_tokenizer.texts_to_sequences(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_train = max(train_tweets, key=lambda sentence: len(word_tokenize(sentence)))\n",
    "length_long_sentence = len(word_tokenize(longest_train))\n",
    "padded_sentences = pad_sequences(embed(train_tweets), length_long_sentence, padding='post')\n",
    "test_sentences = pad_sequences(\n",
    "    embed(test_tweets), \n",
    "    length_long_sentence,\n",
    "    padding='post'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = dict()\n",
    "embedding_dim = 100\n",
    "with open('G:/My Drive/Academia/MSDS/Machine Learning Specialization/DTSA5511 Deep Learning/glove.twitter.27B/glove.twitter.27B.100d.txt', encoding='utf-8') as fp:\n",
    "    for line in fp.readlines():\n",
    "        records = line.split()\n",
    "        word = records[0]\n",
    "        vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "        embeddings_dict [word] = vector_dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.21063   , -0.010992  , -0.17552   , ..., -0.37547001,\n",
       "         0.58029002,  0.16067   ],\n",
       "       [ 0.066373  ,  1.09249997, -0.59674001, ...,  0.040076  ,\n",
       "        -0.12083   , -0.1785    ],\n",
       "       ...,\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.20201001,  0.53070003, -0.15939   , ..., -0.44692001,\n",
       "         0.20909999, -0.4914    ],\n",
       "       [-0.076711  , -0.77710998, -0.75962001, ...,  0.23106   ,\n",
       "         0.09527   , -0.15951   ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Glove dictionary. Others will be initialized to 0.\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_length, embedding_dim))\n",
    "\n",
    "for word, index in word_tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "        \n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    padded_sentences, \n",
    "    train_target, \n",
    "    test_size=0.25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_create_model():\n",
    "    model_path = 'dt_model_final.keras'\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load existing model\n",
    "        print(\"Attempting to load existing model...\")\n",
    "        model = load_model(model_path)\n",
    "        print(\"Existing model loaded successfully!\")\n",
    "        \n",
    "        # Optionally load training history\n",
    "        try:\n",
    "            with open('training_history.json', 'r') as f:\n",
    "                history = json.load(f)\n",
    "            print(\"Training history loaded successfully!\")\n",
    "        except FileNotFoundError:\n",
    "            history = None\n",
    "            print(\"No training history found.\")\n",
    "            \n",
    "        return model, history, False  # False indicates no training needed\n",
    "        \n",
    "    except (OSError, IOError, ValueError):\n",
    "        print(\"No existing model found. Creating new model...\")\n",
    "        model = glove_lstm()\n",
    "        return model, None, True  # True indicates training is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_lstm():\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(\n",
    "        vocab_length,\n",
    "        embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=length_long_sentence,\n",
    "        trainable=False\n",
    "    ))\n",
    "    \n",
    "    # Add LSTM layers\n",
    "    model.add(tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True, dropout=0.2)))\n",
    "    model.add(tf.keras.layers.Bidirectional(LSTM(64, dropout=0.2)))\n",
    "    \n",
    "    # Add Dense layers\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'dt_model_final.keras'\n",
    "model = tf.keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load existing model...\n",
      "Existing model loaded successfully!\n",
      "Training history loaded successfully!\n",
      "Using pre-trained model. Skipping training phase.\n"
     ]
    }
   ],
   "source": [
    "# Call our Function to see if a model exists\n",
    "\n",
    "model, history, needs_training = load_or_create_model()\n",
    "\n",
    "if needs_training:\n",
    "    # Update the checkpoint to save in .keras format \n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'dt_model_final.keras',  \n",
    "        monitor='val_loss',\n",
    "        verbose=1,\n",
    "        save_best_only=True\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        verbose=1,\n",
    "        patience=5,\n",
    "        min_lr=0.001\n",
    "    )\n",
    "\n",
    "    # Save training history\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1,\n",
    "        callbacks=[reduce_lr, checkpoint]\n",
    "    )\n",
    "\n",
    "    # Save to file\n",
    "    with open('training_history.json', 'w') as f:\n",
    "        json.dump(history.history, f)\n",
    "\n",
    "    # Save the final model\n",
    "    model.save('dt_model_final.keras')\n",
    "else:\n",
    "    print(\"Using pre-trained model. Skipping training phase.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(\n",
    "        vocab_length,\n",
    "        embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=length_long_sentence,\n",
    "        trainable=False\n",
    "    ))\n",
    "    \n",
    "    # Add LSTM layers with tunable parameters\n",
    "    model.add(tf.keras.layers.Bidirectional(\n",
    "        LSTM(hp.Int('lstm_units_1', min_value=64, max_value=128, step=32), \n",
    "        return_sequences=True, \n",
    "        dropout=hp.Float('dropout_1', min_value=0.15, max_value=0.25, step=0.1))\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Bidirectional(\n",
    "        LSTM(hp.Int('lstm_units_2', min_value=32, max_value=64, step=16), \n",
    "        dropout=hp.Float('dropout_2', min_value=0.15, max_value=0.25, step=0.1))\n",
    "    ))\n",
    "    \n",
    "    # Add Dense layers\n",
    "    model.add(Dense(hp.Int('dense_units', min_value=16, max_value=64, step=16), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.1, max_value=0.3, step=0.1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=hp.Choice('optimizer', ['adam', 'rmsprop']),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', f1_score_metric]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 03m 18s]\n",
      "val_f1_score_metric: 0.4059727092583974\n",
      "\n",
      "Best val_f1_score_metric So Far: 0.4357193609078725\n",
      "Total elapsed time: 00h 30m 32s\n"
     ]
    }
   ],
   "source": [
    "objective_f1 = keras_tuner.Objective('val_f1_score_metric', \"max\")\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=objective_f1,\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='tuner_results',\n",
    "    project_name='disaster_tweets'\n",
    ")\n",
    "\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7969 - loss: 0.5079\n",
      "Loss: 0.5049303770065308\n",
      "Accuracy: 0.7998949289321899\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print('Loss:', loss)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# Get predictions as probabilities\n",
    "predictions = model.predict(test_sentences)\n",
    "# Convert to binary (0 or 1) predictions using 0.5 threshold\n",
    "submission.target = (predictions > 0.5).astype(int)\n",
    "submission.to_csv(\"submission-v2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n",
    "\n",
    "This one was a\n",
    "https://www.kaggle.com/code/mariapushkareva/nlp-disaster-tweets-with-glove-and-lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
